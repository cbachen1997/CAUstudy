{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZB_semitriplet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNM5yGY1FbMoyNpXF8dsEl/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbachen1997/CAUstudy/blob/master/ZB_semitriplet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsePsStYyYS6",
        "outputId": "56808556-7654-47c3-99b7-bf8855891f9c"
      },
      "source": [
        "#挂载谷歌云盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "####google云盘授权#####\n",
        "##每个notebook执行一次###\n",
        "__author__='CBA'\n",
        "from google.colab import drive\n",
        "\n",
        "#增加PyDrive操作库\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#授权登录\n",
        "auth.authenticate_user()\n",
        "gauth=GoogleAuth()\n",
        "gauth.credentials=GoogleCredentials.get_application_default()\n",
        "drive=GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pv0s4_Pyfm3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "import keras as K\n",
        "import keras.layers as L\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import h5py\n",
        "import argparse \n",
        "import random\n",
        "import cv2\n",
        "from tqdm import *\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LambdaCallback\n",
        "# from keras.callbacks import TensorBoard\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from __future__ import print_function, division\n",
        "from keras.layers import *\n",
        "# from keras.layers import Dense,Dropout\n",
        "from sklearn.utils import shuffle\n",
        "from keras.layers.core import Flatten\n",
        "from keras.utils import to_categorical\n",
        "# from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
        "# from keras.layers.core import Lambda\n",
        "from keras import backend as Kb\n",
        "from collections import Counter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikNJBsr98Tn_"
      },
      "source": [
        "#全局参数\n",
        "inputshape=(11,11,12)\n",
        "num=10#每类抽10个\n",
        "alpha=0.05\n",
        "#标签样本数据npy\n",
        "scale=1000#每次抽取的无标签数量构建伪样本池\n",
        "topk=25#ranking前k个\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRTOD8j1z5AA"
      },
      "source": [
        "from scipy.ndimage.interpolation import map_coordinates as sp_map_coordinates\n",
        "def tf_flatten(a):\n",
        "    \"\"\"Flatten tensor\"\"\"\n",
        "    return tf.reshape(a, [-1])\n",
        "\n",
        "\n",
        "def tf_repeat(a, repeats, axis=0):\n",
        "    \"\"\"TensorFlow version of np.repeat for 1D\"\"\"\n",
        "    # https://github.com/tensorflow/tensorflow/issues/8521\n",
        "    assert len(a.get_shape()) == 1\n",
        "\n",
        "    a = tf.expand_dims(a, -1)\n",
        "    a = tf.tile(a, [1, repeats])\n",
        "    a = tf_flatten(a)\n",
        "    return a\n",
        "\n",
        "\n",
        "def tf_repeat_2d(a, repeats):\n",
        "    \"\"\"Tensorflow version of np.repeat for 2D\"\"\"\n",
        "\n",
        "    assert len(a.get_shape()) == 2\n",
        "    a = tf.expand_dims(a, 0)\n",
        "    a = tf.tile(a, [repeats, 1, 1])\n",
        "    return a\n",
        "\n",
        "\n",
        "def tf_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Tensorflow verion of scipy.ndimage.map_coordinates\n",
        "\n",
        "    Note that coords is transposed and only 2D is supported\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (s, s)\n",
        "    coords : tf.Tensor. shape = (n_points, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    assert order == 1\n",
        "\n",
        "    coords_lt = tf.cast(tf.floor(coords), 'int32')\n",
        "    #coords_rb = tf.cast(tf.ceil(coords), 'int32')\n",
        "    coords_rb = tf.cast(tf.math.ceil(coords), 'int32')\n",
        "    coords_lb = tf.stack([coords_lt[:, 0], coords_rb[:, 1]], axis=1)\n",
        "    coords_rt = tf.stack([coords_rb[:, 0], coords_lt[:, 1]], axis=1)\n",
        "\n",
        "    vals_lt = tf.gather_nd(input, coords_lt)\n",
        "    vals_rb = tf.gather_nd(input, coords_rb)\n",
        "    vals_lb = tf.gather_nd(input, coords_lb)\n",
        "    vals_rt = tf.gather_nd(input, coords_rt)\n",
        "\n",
        "    coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n",
        "    vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, 0]\n",
        "    vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, 0]\n",
        "    mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, 1]\n",
        "\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_coordinates(inputs, coords):\n",
        "    \"\"\"Reference implementation for batch_map_coordinates\"\"\"\n",
        "    coords = coords.clip(0, inputs.shape[1] - 1)\n",
        "    mapped_vals = np.array([\n",
        "        sp_map_coordinates(input, coord.T, mode='nearest', order=1)\n",
        "        for input, coord in zip(inputs, coords)\n",
        "    ])\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def tf_batch_map_coordinates(input, coords, order=1):\n",
        "    \"\"\"Batch version of tf_map_coordinates\n",
        "\n",
        "    Only supports 2D feature maps\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tf.Tensor. shape = (b, s, s)\n",
        "    coords : tf.Tensor. shape = (b, n_points, 2)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tf.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = tf.shape(input)\n",
        "    batch_size = input_shape[0]\n",
        "    input_size = input_shape[1]\n",
        "    n_coords = tf.shape(coords)[1]\n",
        "\n",
        "    coords = tf.clip_by_value(coords, 0, tf.cast(input_size, 'float32') - 1)\n",
        "    coords_lt = tf.cast(tf.floor(coords), 'int32')\n",
        "    #coords_rb = tf.cast(tf.ceil(coords), 'int32')\n",
        "    coords_rb = tf.cast(tf.math.ceil(coords), 'int32')\n",
        "    coords_lb = tf.stack([coords_lt[..., 0], coords_rb[..., 1]], axis=-1)\n",
        "    coords_rt = tf.stack([coords_rb[..., 0], coords_lt[..., 1]], axis=-1)\n",
        "\n",
        "    idx = tf_repeat(tf.range(batch_size), n_coords)\n",
        "\n",
        "    def _get_vals_by_coords(input, coords):\n",
        "        indices = tf.stack([\n",
        "            idx, tf_flatten(coords[..., 0]), tf_flatten(coords[..., 1])\n",
        "        ], axis=-1)\n",
        "        vals = tf.gather_nd(input, indices)\n",
        "        vals = tf.reshape(vals, (batch_size, n_coords))\n",
        "        return vals\n",
        "\n",
        "    vals_lt = _get_vals_by_coords(input, coords_lt)\n",
        "    vals_rb = _get_vals_by_coords(input, coords_rb)\n",
        "    vals_lb = _get_vals_by_coords(input, coords_lb)\n",
        "    vals_rt = _get_vals_by_coords(input, coords_rt)\n",
        "\n",
        "    coords_offset_lt = coords - tf.cast(coords_lt, 'float32')\n",
        "    vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[..., 0]\n",
        "    vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[..., 0]\n",
        "    mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[..., 1]\n",
        "\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def sp_batch_map_offsets(input, offsets):\n",
        "    \"\"\"Reference implementation for tf_batch_map_offsets\"\"\"\n",
        "\n",
        "    batch_size = input.shape[0]\n",
        "    input_size = input.shape[1]\n",
        "\n",
        "    offsets = offsets.reshape(batch_size, -1, 2)\n",
        "    grid = np.stack(np.mgrid[:input_size, :input_size], -1).reshape(-1, 2)\n",
        "    grid = np.repeat([grid], batch_size, axis=0)\n",
        "    coords = offsets + grid\n",
        "    coords = coords.clip(0, input_size - 1)\n",
        "\n",
        "    mapped_vals = sp_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals\n",
        "\n",
        "\n",
        "def tf_batch_map_offsets(input, offsets, order=1):\n",
        "    \"\"\"Batch map offsets into input\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    input : tf.Tensor. shape = (b, s, s)\n",
        "    offsets: tf.Tensor. shape = (b, s, s, 2)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tf.Tensor. shape = (b, s, s)\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = tf.shape(input)\n",
        "    batch_size = input_shape[0]\n",
        "    input_size = input_shape[1]\n",
        "\n",
        "    offsets = tf.reshape(offsets, (batch_size, -1, 2))\n",
        "    grid = tf.meshgrid(\n",
        "        tf.range(input_size), tf.range(input_size), indexing='ij'\n",
        "    )\n",
        "    grid = tf.stack(grid, axis=-1)\n",
        "    grid = tf.cast(grid, 'float32')\n",
        "    grid = tf.reshape(grid, (-1, 2))\n",
        "    grid = tf_repeat_2d(grid, batch_size)\n",
        "    coords = offsets + grid\n",
        "\n",
        "    mapped_vals = tf_batch_map_coordinates(input, coords)\n",
        "    return mapped_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YbRJ9Kz8_I"
      },
      "source": [
        "class ConvOffset2D(Conv2D):\n",
        "    \"\"\"ConvOffset2D\n",
        "\n",
        "    Convolutional layer responsible for learning the 2D offsets and output the\n",
        "    deformed feature map using bilinear interpolation\n",
        "\n",
        "    Note that this layer does not perform convolution on the deformed feature\n",
        "    map. See get_deform_cnn in cnn.py for usage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, init_normal_stddev=0.01, **kwargs):\n",
        "        \"\"\"Init\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filters : int\n",
        "            Number of channel of the input feature map\n",
        "        init_normal_stddev : float\n",
        "            Normal kernel initialization\n",
        "        **kwargs:\n",
        "            Pass to superclass. See Con2D layer in Keras\n",
        "        \"\"\"\n",
        "\n",
        "        self.filters = filters\n",
        "        super(ConvOffset2D, self).__init__(\n",
        "            self.filters * 2, (3, 3), padding='same', use_bias=False,\n",
        "            kernel_initializer=K.initializers.RandomNormal(0, init_normal_stddev),\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"Return the deformed featured map\"\"\"\n",
        "        x_shape = x.get_shape()\n",
        "        offsets = super(ConvOffset2D, self).call(x)\n",
        "\n",
        "        # offsets: (b*c, h, w, 2)\n",
        "        offsets = self._to_bc_h_w_2(offsets, x_shape)\n",
        "\n",
        "        # x: (b*c, h, w)\n",
        "        x = self._to_bc_h_w(x, x_shape)\n",
        "\n",
        "        # X_offset: (b*c, h, w)\n",
        "        x_offset = tf_batch_map_offsets(x, offsets)\n",
        "\n",
        "        # x_offset: (b, h, w, c)\n",
        "        x_offset = self._to_b_h_w_c(x_offset, x_shape)\n",
        "\n",
        "        return x_offset\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"Output shape is the same as input shape\n",
        "\n",
        "        Because this layer does only the deformation part\n",
        "        \"\"\"\n",
        "        return input_shape\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w_2(x, x_shape):\n",
        "        \"\"\"(b, h, w, 2c) -> (b*c, h, w, 2)\"\"\"\n",
        "        x = tf.transpose(x, [0, 3, 1, 2])\n",
        "        x = tf.reshape(x, (-1, int(x_shape[1]), int(x_shape[2]), 2))\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_bc_h_w(x, x_shape):\n",
        "        \"\"\"(b, h, w, c) -> (b*c, h, w)\"\"\"\n",
        "        x = tf.transpose(x, [0, 3, 1, 2])\n",
        "        x = tf.reshape(x, (-1, int(x_shape[1]), int(x_shape[2])))\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_b_h_w_c(x, x_shape):\n",
        "        \"\"\"(b*c, h, w) -> (b, h, w, c)\"\"\"\n",
        "        x = tf.reshape(\n",
        "            x, (-1, int(x_shape[3]), int(x_shape[1]), int(x_shape[2]))\n",
        "        )\n",
        "        x = tf.transpose(x, [0, 2, 3, 1])\n",
        "        return x\n",
        "def BN_LeakyReLU(input):\n",
        "    \n",
        "    norm = L.BatchNormalization(axis=-1)(input)\n",
        "    output = L.advanced_activations.LeakyReLU(alpha=0.2)(norm)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-SeEt340Bho"
      },
      "source": [
        "######################################backbone部分######################################\n",
        "def feature_extraction_CNN(input_shape, n_filters=64):\n",
        "    # X_input=L.Input(input_shape)\n",
        "    conv1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(input_shape)\n",
        "   \n",
        "    conv1 = BN_LeakyReLU(conv1)\n",
        "    conv1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(conv1)\n",
        "    conv1 = BN_LeakyReLU(conv1)\n",
        "    \n",
        "    pool1 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv1)\n",
        "    # print(pool1)\n",
        "    #可变形block A1\n",
        "   \n",
        "    offset_conv2_1 = ConvOffset2D(2*n_filters)(pool1)\n",
        "   \n",
        "    conv2_1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_1)\n",
        "    conv2_1 = BN_LeakyReLU(conv2_1)\n",
        "    \n",
        "    offset_conv2_2 = ConvOffset2D(n_filters)(conv2_1)\n",
        "    conv2_2 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_2)\n",
        "    conv2_2 = BN_LeakyReLU(conv2_2)\n",
        "    \n",
        "    offset_conv2_3 = ConvOffset2D(int(0.5*n_filters))(conv2_2)\n",
        "    conv2_3 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_3)\n",
        "    conv2_3 = BN_LeakyReLU(conv2_3)\n",
        "    \n",
        "    conv2_4 = L.concatenate([conv2_1, conv2_2, conv2_3], axis=-1)\n",
        "    # print(conv2_4)\n",
        "    conv2_5 = L.merge.add([conv2_4, pool1])\n",
        "\n",
        "    #可变形block A2\n",
        "    offset_conv2_6 = ConvOffset2D(2*n_filters)(conv2_5)\n",
        "    conv2_6 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_6)\n",
        "    conv2_6 = BN_LeakyReLU(conv2_6)\n",
        "    \n",
        "    offset_conv2_7 = ConvOffset2D(n_filters)(conv2_6)\n",
        "    conv2_7 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_7)\n",
        "    conv2_7 = BN_LeakyReLU(conv2_7)\n",
        "    \n",
        "    offset_conv2_8 = ConvOffset2D(int(0.5*n_filters))(conv2_7)\n",
        "    conv2_8 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_8)\n",
        "    conv2_8 = BN_LeakyReLU(conv2_8)\n",
        "    \n",
        "    conv2_9 = L.concatenate([conv2_6, conv2_7, conv2_8], axis=-1)\n",
        "    \n",
        "    conv2_10 = L.merge.add([conv2_9, conv2_5])\n",
        "    \n",
        "    \n",
        "    #pool2 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv2_5)\n",
        "    \n",
        "    conv3 = L.Conv2D(4*n_filters, (3, 3), padding='valid', strides=(2, 2), kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(conv2_5)\n",
        "    \n",
        "    offset_conv3_1 = ConvOffset2D(4*n_filters)(conv3)\n",
        "    \n",
        "    conv3_1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_1)\n",
        "    conv3_1 = BN_LeakyReLU(conv3_1)\n",
        "    \n",
        "    offset_conv3_2 = ConvOffset2D(2*n_filters)(conv3_1)\n",
        "    conv3_2 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_2)\n",
        "    conv3_2 = BN_LeakyReLU(conv3_2)\n",
        "    offset_conv3_3 = ConvOffset2D(n_filters)(conv3_2)\n",
        "    conv3_3 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_3)\n",
        "    conv3_3 = BN_LeakyReLU(conv3_3)\n",
        "    \n",
        "    conv3_4 = L.concatenate([conv3_1, conv3_2, conv3_3], axis=-1)\n",
        "    \n",
        "    conv3_5 = L.merge.add([conv3_4, conv3])\n",
        " \n",
        "    #改了一下4*\n",
        "    offset_conv3_6 = ConvOffset2D(n_filters)(conv3_2)\n",
        "    \n",
        "    conv3_6 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_6)\n",
        "    conv3_6 = BN_LeakyReLU(conv3_6)\n",
        "    \n",
        "    offset_conv3_7 = ConvOffset2D(2*n_filters)(conv3_6)\n",
        "    conv3_7 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_7)\n",
        "    conv3_7 = BN_LeakyReLU(conv3_7)\n",
        "    \n",
        "    offset_conv3_8 = ConvOffset2D(n_filters)(conv3_7)\n",
        "    conv3_8 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_8)\n",
        "    conv3_8 = BN_LeakyReLU(conv3_8)\n",
        "    \n",
        "    conv3_9 = L.concatenate([conv3_6, conv3_7, conv3_8], axis=-1)\n",
        "    \n",
        "    conv3_10 = L.merge.add([conv3_9, conv3_5])\n",
        "    \n",
        "    \n",
        "    #pool3 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv3)\n",
        "    #输出128维计算度量\n",
        "    conv4 = L.Conv2D(n_filters*2, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(conv3_10)\n",
        "    # print(conv4)\n",
        "    gap = L.GlobalAvgPool2D()(conv4)\n",
        "    # print(gap)\n",
        "    #增加功能\n",
        "    X = Dense(1024,kernel_regularizer=regularizers.l2(0.01),name='dense_layer1')(gap)\n",
        "    X1 = Dropout(rate=0.5)(X)\n",
        "    X2 = Dense(256,kernel_regularizer=regularizers.l2(0.01),name='dense_layer2')(X1)\n",
        "    spatial_result = X2\n",
        "    print('输出向量维度：' + str(spatial_result.shape))\n",
        "    return spatial_result\n",
        "def feature_extraction_CNN_tri(input_shape, n_filters=64):\n",
        "    X_input=L.Input(input_shape)\n",
        "    conv1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(X_input)\n",
        "   \n",
        "    conv1 = BN_LeakyReLU(conv1)\n",
        "    conv1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(conv1)\n",
        "    conv1 = BN_LeakyReLU(conv1)\n",
        "    \n",
        "    pool1 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv1)\n",
        "    # print(pool1)\n",
        "    #可变形block A1\n",
        "   \n",
        "    offset_conv2_1 = ConvOffset2D(2*n_filters)(pool1)\n",
        "   \n",
        "    conv2_1 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_1)\n",
        "    conv2_1 = BN_LeakyReLU(conv2_1)\n",
        "    \n",
        "    offset_conv2_2 = ConvOffset2D(n_filters)(conv2_1)\n",
        "    conv2_2 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_2)\n",
        "    conv2_2 = BN_LeakyReLU(conv2_2)\n",
        "    \n",
        "    offset_conv2_3 = ConvOffset2D(int(0.5*n_filters))(conv2_2)\n",
        "    conv2_3 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_3)\n",
        "    conv2_3 = BN_LeakyReLU(conv2_3)\n",
        "    \n",
        "    conv2_4 = L.concatenate([conv2_1, conv2_2, conv2_3], axis=-1)\n",
        "    # print(conv2_4)\n",
        "    conv2_5 = L.merge.add([conv2_4, pool1])\n",
        "\n",
        "    #可变形block A2\n",
        "    offset_conv2_6 = ConvOffset2D(2*n_filters)(conv2_5)\n",
        "    conv2_6 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_6)\n",
        "    conv2_6 = BN_LeakyReLU(conv2_6)\n",
        "    \n",
        "    offset_conv2_7 = ConvOffset2D(n_filters)(conv2_6)\n",
        "    conv2_7 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_7)\n",
        "    conv2_7 = BN_LeakyReLU(conv2_7)\n",
        "    \n",
        "    offset_conv2_8 = ConvOffset2D(int(0.5*n_filters))(conv2_7)\n",
        "    conv2_8 = L.Conv2D(int(0.5*n_filters), (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv2_8)\n",
        "    conv2_8 = BN_LeakyReLU(conv2_8)\n",
        "    \n",
        "    conv2_9 = L.concatenate([conv2_6, conv2_7, conv2_8], axis=-1)\n",
        "    \n",
        "    conv2_10 = L.merge.add([conv2_9, conv2_5])\n",
        "    \n",
        "    \n",
        "    #pool2 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv2_5)\n",
        "    \n",
        "    conv3 = L.Conv2D(4*n_filters, (3, 3), padding='valid', strides=(2, 2), kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(conv2_5)\n",
        "    \n",
        "    offset_conv3_1 = ConvOffset2D(4*n_filters)(conv3)\n",
        "    \n",
        "    conv3_1 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_1)\n",
        "    conv3_1 = BN_LeakyReLU(conv3_1)\n",
        "    \n",
        "    offset_conv3_2 = ConvOffset2D(2*n_filters)(conv3_1)\n",
        "    conv3_2 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_2)\n",
        "    conv3_2 = BN_LeakyReLU(conv3_2)\n",
        "    offset_conv3_3 = ConvOffset2D(n_filters)(conv3_2)\n",
        "    conv3_3 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_3)\n",
        "    conv3_3 = BN_LeakyReLU(conv3_3)\n",
        "    \n",
        "    conv3_4 = L.concatenate([conv3_1, conv3_2, conv3_3], axis=-1)\n",
        "    \n",
        "    conv3_5 = L.merge.add([conv3_4, conv3])\n",
        " \n",
        "    #改了一下4*\n",
        "    offset_conv3_6 = ConvOffset2D(n_filters)(conv3_2)\n",
        "    \n",
        "    conv3_6 = L.Conv2D(2*n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_6)\n",
        "    conv3_6 = BN_LeakyReLU(conv3_6)\n",
        "    \n",
        "    offset_conv3_7 = ConvOffset2D(2*n_filters)(conv3_6)\n",
        "    conv3_7 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_7)\n",
        "    conv3_7 = BN_LeakyReLU(conv3_7)\n",
        "    \n",
        "    offset_conv3_8 = ConvOffset2D(n_filters)(conv3_7)\n",
        "    conv3_8 = L.Conv2D(n_filters, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(offset_conv3_8)\n",
        "    conv3_8 = BN_LeakyReLU(conv3_8)\n",
        "    \n",
        "    conv3_9 = L.concatenate([conv3_6, conv3_7, conv3_8], axis=-1)\n",
        "    \n",
        "    conv3_10 = L.merge.add([conv3_9, conv3_5])\n",
        "    \n",
        "    \n",
        "    #pool3 = L.MaxPool2D(pool_size=(2, 2),padding='same')(conv3)\n",
        "    #输出128维计算度量\n",
        "    conv4 = L.Conv2D(n_filters*2, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                    kernel_regularizer=regularizers.l2(0.0001))(conv3_10)\n",
        "    # print(conv4)\n",
        "    gap = L.GlobalAvgPool2D()(conv4)\n",
        "    # print(gap)\n",
        "    #增加功能\n",
        "    X = Dense(1024,kernel_regularizer=regularizers.l2(0.01),name='dense_layer1')(gap)\n",
        "    X1 = Dropout(rate=0.5)(X)\n",
        "    X2 = Dense(256,kernel_regularizer=regularizers.l2(0.01),name='dense_layer2')(X1)\n",
        "    spatial_result = X2\n",
        "    print('输出向量维度：' + str(spatial_result.shape))\n",
        "    model = Model(inputs=X_input,outputs=spatial_result,name='triplet_Model')\n",
        "    return model\n",
        "    # return spatial_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qmhHN1w0F7I"
      },
      "source": [
        "#1 抽取样本(每类num个)\n",
        "def sampling(dataset_X,dataset_Y,num):\n",
        "  X = []\n",
        "  Y = []\n",
        "  flag = 0\n",
        "  for i in range(1,8):#7类\n",
        "    classes = i\n",
        "    # print('当前类别为：'+ str(classes))\n",
        "    current_dataset = dataset_X[dataset_Y == classes]\n",
        "    #随机抽\n",
        "    np.random.shuffle(current_dataset)\n",
        "    random_samples = current_dataset[0:num]\n",
        "    if flag == 0 :\n",
        "      random_samples = random_samples.tolist()\n",
        "      X.append(random_samples)\n",
        "      Y = Y + [classes for j in range(0,num)]\n",
        "      X = np.asarray(X[0],dtype=float)\n",
        "      flag = 1\n",
        "    else:\n",
        "      X = np.concatenate((X,random_samples),axis=0)\n",
        "      Y = Y + [classes for j in range(0,num)]\n",
        "  print('当前采样集大小:'+str(X.shape))\n",
        "  Y = np.asarray(Y ,dtype = int)\n",
        "  return X,Y#Y没啥用\n",
        "def sampling_unlabel(unlabeled_X,scale):\n",
        "  # np.random.shuffle(unlabeled_X)\n",
        "  pe = np.random.permutation(unlabeled_X.shape[0])\n",
        "  # print(pe)\n",
        "  unlabeled_X =unlabeled_X[pe]\n",
        "  unlabeled_X = unlabeled_X[0:scale,:]\n",
        "  # print(unlabeled_X.shape)\n",
        "  return unlabeled_X\n",
        "#2 embedding\n",
        "# def embedding(datanpy,model):\n",
        "#   length  = datanpy.shape[0]\n",
        "#   embed_array = np.zeros((datanpy.shape[0],256))\n",
        "#   for i in range(length):\n",
        "#     embedding=model.predict_on_batch(datanpy)\n",
        "#   return embedding\n",
        "#3 构建困难三元组\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.spatial.distance import pdist\n",
        "def get_hard_triplet(samples_X,samples_Y,model):\n",
        "  \"\"\"\n",
        "  samples:采样结果\n",
        "  model:模型\n",
        "  \"\"\"\n",
        "  num_ = samples_X.shape[0]\n",
        "  embed = model.predict(samples_X)\n",
        "  hard_triplet=np.zeros((num_,3,samples_X.shape[1],samples_X.shape[2],samples_X.shape[3]))\n",
        "  count = 0 \n",
        "  # print('构建困难三元组ing...')\n",
        "  for i in range(0,num_):\n",
        "    classes = samples_Y[i]\n",
        "    anchor = embed[i,:]\n",
        "    ##################构建困难正样本##################\n",
        "    #构建除anchor以外的pos样本\n",
        "    pos_embed = embed[samples_Y == classes]\n",
        "    pos_embed = np.delete(pos_embed,np.where(pos_embed == anchor)[0][0],axis=0)\n",
        "    #reshape便于计算\n",
        "    anchor = anchor.reshape((1,-1))\n",
        "    #计算距离\n",
        "    anchor = np.repeat(anchor, pos_embed.shape[0], axis=0)\n",
        "    temp_dis=np.sum((anchor-pos_embed)*(anchor-pos_embed),axis=1)\n",
        "    loc_p=temp_dis.argmax()#类内最大距离\n",
        "    hard_pos=samples_X[loc_p,:,:,:]\n",
        "    ##################构建困难负样本##################\n",
        "    neg_embed = embed[samples_Y != classes]\n",
        "    anchor = embed[i,:]\n",
        "    anchor = anchor.reshape((1,-1))\n",
        "    anchor = np.repeat(anchor, neg_embed.shape[0], axis=0)\n",
        "    temp_dis=np.sum((anchor-neg_embed)*(anchor-neg_embed),axis=1)\n",
        "    loc_n=temp_dis.argmin()#类间最小距离\n",
        "    hard_neg=samples_X[loc_n,:,:,:]\n",
        "    hard_triplet[i,0,:,:,:]=samples_X[i,:,:,:]\n",
        "    hard_triplet[i,1,:,:,:]=hard_pos\n",
        "    hard_triplet[i,2,:,:,:]=hard_neg\n",
        "  return hard_triplet\n",
        "#3.1 构建困难三元组嵌套\n",
        "def process_triplet(data_X,label,model,num=num):\n",
        "  #抽样\n",
        "  sample_X,sample_Y=sampling(data_X,label,num=num)\n",
        "  #获取困难三元组\n",
        "  hard_tri=get_hard_triplet(sample_X,sample_Y,model)\n",
        "  # print('困难三元组构建完成')\n",
        "  return hard_tri"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOJbAoHs0L09"
      },
      "source": [
        "def mixup(train_X,label,unlabeled,model,scale=scale):\n",
        "  # Smodel.load_weights(weight_)\n",
        "  #随机从无标签池中抽取样本\n",
        "  temp_unlabeled=sampling_unlabel(unlabeled,scale)\n",
        "  train_embed = model.predict(train_X)\n",
        "  #开始mixup\n",
        "  # count_all_label=[]\n",
        "  plabel_all = []\n",
        "  acc_all = []\n",
        "  for i in range(len(temp_unlabeled)):\n",
        "    data_enhanced=np.zeros((7,11,11,12))\n",
        "    #减少增强次数\n",
        "    # data_enhanced=np.zeros((6,11,11,7))\n",
        "    data_enhanced[0,]=unlabeled[i,]\n",
        "    #水平垂直对角翻转\n",
        "    data_enhanced[1,]=np.flip(unlabeled[i,],axis=0)\n",
        "    data_enhanced[2,]=np.flip(unlabeled[i,],axis=1)\n",
        "    data_enhanced[3,]=np.flip(unlabeled[i,],axis=2)\n",
        "    #增加噪声(轻噪，重噪)\n",
        "    noise = np.random.normal(0.0, 0.01, size=unlabeled[i,].shape)\n",
        "    data_enhanced[4,]=i+noise\n",
        "    noise = np.random.normal(0.0, 0.05, size=unlabeled[i,].shape)\n",
        "    data_enhanced[5,]=i+noise\n",
        "    #随机旋转\n",
        "    k = np.random.randint(4)\n",
        "    data_enhanced[6,]=np.rot90(unlabeled[i,],k=k)\n",
        "    #增强结果的embed\n",
        "    enhance_embed=model.predict(data_enhanced)\n",
        "    #计算度量获取伪标签\n",
        "    pseudo_label=[]\n",
        "    for j in range(len(enhance_embed)):\n",
        "      #扩充向量维度，计算距离，找最小距离\n",
        "      dis_list=[]\n",
        "      #伪标签获取\n",
        "      dist_mat = tf.reduce_sum(tf.square(tf.subtract(enhance_embed[j,].reshape((1,-1)),train_embed)),axis=-1)\n",
        "      index = np.where(dist_mat == np.min(dist_mat))[0][0]\n",
        "      # print(index)\n",
        "      classes = label[index]    \n",
        "      pseudo_label.append(classes)\n",
        "    counts = np.bincount(pseudo_label)\n",
        "    plabel= np.argmax(counts)\n",
        "    acc=counts[plabel]/len(pseudo_label)\n",
        "    plabel_all.append(plabel)\n",
        "    acc_all.append(acc)\n",
        "\n",
        "  #返回batch所有无标签样本和对应伪标签及概率\n",
        "  plabel_all = np.asarray(plabel_all,dtype= int)\n",
        "  acc_all = np.asarray(acc_all,dtype=float)\n",
        "  return temp_unlabeled,plabel_all,acc_all\n",
        "#获取pseudo triplet\n",
        "def ranking(unlabeled_sample,p_label,p_prob,topk=topk):\n",
        "  pseudo_triplet = []\n",
        "  pseudo_label = []\n",
        "  flag = 0 \n",
        "  # ranksample = np.zeros((topk*))\n",
        "  for i in range(1,8):\n",
        "    classes = i\n",
        "    # temp_label = p_label[p_label == classes]\n",
        "    temp_prob = p_prob[(p_label == classes)& (p_prob > 0.57)]\n",
        "    temp_sample = unlabeled_sample[(p_label == classes)& (p_prob > 0.57)]\n",
        "    temp_rank = np.argsort(temp_prob,axis=0)\n",
        "    length = len(temp_rank)\n",
        "    if length >= topk:\n",
        "      for j in range(0,topk):\n",
        "        ranknum = np.where(temp_rank == j)[0][0]\n",
        "        pseudo_triplet.append(temp_sample[ranknum,:])\n",
        "        pseudo_label.append(i)\n",
        "    else:\n",
        "      for j in range(0,length):\n",
        "        ranknum = np.where(temp_rank == j)[0][0]\n",
        "        pseudo_triplet.append(temp_sample[ranknum,:])\n",
        "        pseudo_label.append(i)\n",
        "  final_pseudo_sample = np.array(pseudo_triplet)\n",
        "  final_pseudo_label = np.asarray(pseudo_label,dtype=int)\n",
        "  \n",
        "  return final_pseudo_sample,final_pseudo_label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsYVZS5J0iVF"
      },
      "source": [
        "#解决伪标签全为一类的问题\n",
        "def judge(pseudo_label,prob,num):\n",
        "  #判断随机伪样本是否至少除other类外都大于10num个\n",
        "  count = 0\n",
        "  for i in range(1,8):\n",
        "    temp_prob = prob[pseudo_label == i]\n",
        "    temp_prob = temp_prob[temp_prob > 0.57]\n",
        "    if temp_prob.shape[0] >= num:\n",
        "      count += 1\n",
        "  if count >= 7:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlRIt1qm-r0Q"
      },
      "source": [
        "#三元组损失计算\n",
        "def triplet_loss(triplet,alpha=alpha):\n",
        "  \"\"\"\n",
        "  triplet=三元组\n",
        "  \"\"\"\n",
        "  anchor,positive,negative=triplet[0],triplet[1],triplet[2]\n",
        "  # print(anchor.shape)\n",
        "  #第一步：计算\"anchor\" 与 \"positive\"之间编码的距离，这里需要使用axis=-1\n",
        "  pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)#二范数，-1代表倒数第一个维度即64\n",
        "  #第二步：计算\"anchor\" 与 \"negative\"之间编码的距离，这里需要使用axis=-1\n",
        "  neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
        "  #第三步：减去之前的两个距离，然后加上alpha\n",
        "  basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
        "  #通过取带零的最大值和对训练样本的求和来计算整个公式\n",
        "  loss = tf.reduce_sum(tf.maximum(basic_loss,0))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CE6RJMU0V7H"
      },
      "source": [
        "def RPmodel(model,alpha=0.05,P_weight=0.2):\n",
        "  \"\"\"\n",
        "  model:网络backbone\n",
        "  alpha:margin参数\n",
        "  P_weight:伪样本损失权重\n",
        "  return:返回算好的损失\n",
        "  \"\"\"\n",
        "  #真实样本损失\n",
        "  y_pred_real=[]\n",
        "  real_anchor = Input(inputshape)\n",
        "  real_positive = Input(inputshape)\n",
        "  real_negative = Input(inputshape)\n",
        "  r_anchor_out = Model(real_anchor, model(real_anchor))\n",
        "  r_positive_out = Model(real_positive, model(real_positive))\n",
        "  r_negative_out = Model(real_negative, model(real_negative))\n",
        "  y_pred_real.append(r_anchor_out.output)\n",
        "  y_pred_real.append(r_positive_out.output)\n",
        "  y_pred_real.append(r_negative_out.output)\n",
        "  loss_real = Lambda(lambda x: triplet_loss(x, alpha), output_shape=[1])(y_pred_real)\n",
        "  #伪样本损失\n",
        "  y_pred_pseudo=[]\n",
        "  pseudo_anchor = Input(inputshape)\n",
        "  pseudo_positive = Input(inputshape)\n",
        "  pseudo_negative = Input(inputshape)\n",
        "  p_anchor_out = Model(pseudo_anchor, model(pseudo_anchor))\n",
        "  p_positive_out = Model(pseudo_positive, model(pseudo_positive))\n",
        "  p_negative_out = Model(pseudo_negative, model(pseudo_negative))\n",
        "  y_pred_pseudo.append(p_anchor_out.output)\n",
        "  y_pred_pseudo.append(p_positive_out.output)\n",
        "  y_pred_pseudo.append(p_negative_out.output)\n",
        "  loss_pseudo = Lambda(lambda x: triplet_loss(x, alpha), output_shape=[1])(y_pred_pseudo)\n",
        "  \n",
        "  tripletloss=loss_real+P_weight*loss_pseudo\n",
        "  #输入正负样本的三元组，输出损失\n",
        "  return Model(inputs = [real_anchor,real_positive,real_negative,pseudo_anchor,pseudo_positive,pseudo_negative],outputs=tripletloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB62wxJw0bsi"
      },
      "source": [
        "def train_model(model,train_X,train_Y,unlabeled):#待修改\n",
        "  \"\"\"\n",
        "  model:backbone\n",
        "  X_pos,X_neg:有标签正/负样本\n",
        "  unlabeled：无标签样本池\n",
        "  \"\"\"\n",
        "  num_times = 20 #训练num_times次，每次epoch轮\n",
        "  P_weight1=0.0\n",
        "  semimodel = RPmodel(model,alpha=0,P_weight = 0.1)\n",
        "  optm = K.optimizers.Adam(lr=1e-4,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01)\n",
        "  semimodel.compile(optimizer=optm,loss=lambda t,p:p)\n",
        "  model.load_weights('/content/gdrive/MyDrive/CM/weight_folder/zhangbei_triplet_weight_alpha=0.h5',by_name=True)\n",
        "  model_checkpoint = ModelCheckpoint('/content/gdrive/MyDrive/CM/weight_folder/zhangbei_triplet_weight_plus_pseudo_alpha=0.h5', monitor= 'loss', verbose=1, \n",
        "                                     save_best_only=True, save_weights_only=True,period=5) \n",
        "  batch_print_callback = LambdaCallback(on_epoch_end=lambda batch,logs: print('验证集精度：'+str(use_val(X_train,Y_train,X_val,Y_val,model))))\n",
        "  early_stopping =EarlyStopping(monitor='loss', patience=5)\n",
        "  for i in tqdm(range(num_times)):\n",
        "    #真实样本困难三元组\n",
        "    real_hard_tri = process_triplet(train_X,train_Y,model,num=5)\n",
        "    print('真实样本困难三元组构建完成')\n",
        "    # print('a')\n",
        "    unlabeled_batch,class_label,class_prob=mixup(train_X,train_Y,unlabeled,model,scale=2000)\n",
        "    # print(class_prob)\n",
        "    while (judge(class_label,class_prob,num=5)==False):\n",
        "      print(\"pseudo labeled failed, retry\")\n",
        "      unlabeled_batch,class_label,class_prob=mixup(train_X,train_Y,unlabeled,model,scale=2000)\n",
        "      # judge(class_label,class_prob,num=10)\n",
        "    print(\"success,continue\")\n",
        "    # pseudo_pos,pseudo_neg=ranking(unlabeled_batch,class_prob)\n",
        "    pseudo_sample,pseudo_label = ranking(unlabeled_batch,class_label,class_prob,topk=5)\n",
        "    # #真实样本池增加\n",
        "    # X_positive = np.concatenate((X_positive,pseudo_pos),axis=0)\n",
        "    # X_negative = np.concatenate((X_negative,pseudo_neg),axis=0)\n",
        "    # print(X_positive.shape)\n",
        "    # print(X_negative.shape)\n",
        "    pseudo_hard_tri=get_hard_triplet(pseudo_sample,pseudo_label,model)\n",
        "    print('伪样本困难三元组构建完成')\n",
        "    # all_tri = np.zeros((real_hard_tri.shape[0],real_hard_tri.shape[1]*2,real_hard_tri.shape[2],real_hard_tri.shape[3],real_hard_tri.shape[4]))\n",
        "    print(real_hard_tri.shape)\n",
        "    print(pseudo_hard_tri.shape)\n",
        "    all_tri = np.concatenate((real_hard_tri,pseudo_hard_tri),axis=1)\n",
        "    print(all_tri.shape)\n",
        "    y_label = np.zeros((len(all_tri),1))#用不着\n",
        "    input_x = [\n",
        "            all_tri[:,0,:,:,:],\n",
        "            all_tri[:,1,:,:,:],\n",
        "            all_tri[:,2,:,:,:],\n",
        "            all_tri[:,3,:,:,:],\n",
        "            all_tri[:,4,:,:,:],\n",
        "            all_tri[:,5,:,:,:]\n",
        "    ]\n",
        "    semimodel.fit(x=input_x,y=y_label,batch_size=25,epochs=50,shuffle=True,callbacks=[model_checkpoint, early_stopping, batch_print_callback])\n",
        "    P_weight1 += 0.025\n",
        "    # #保存训练完成一次的权重，重新映射获取三元组\n",
        "    # model.save('/content/gdrive/MyDrive/CM/recurrent_weight.h5')\n",
        "    # positive_embed = embedding(X_pos,model)\n",
        "    # negative_embed = embedding(X_neg,model)\n",
        "    # model.save('/content/gdrive/MyDrive/CM/weight_folder/new_dataset_triplet_weight_alpha0.05.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8C_CxAM-4VG"
      },
      "source": [
        "def test_predict(train_X,train_Y,test_X,model):\n",
        "  train_X_embed = model.predict(train_X)\n",
        "  test_X_embed = model.predict(test_X)\n",
        "  pred_class = []\n",
        "  for i in range(0,test_X_embed.shape[0]):\n",
        "    #计算到各个类别的距离\n",
        "    dist_mat = tf.reduce_sum(tf.square(tf.subtract(test_X_embed[i],train_X_embed)),axis=-1)\n",
        "    index = int(np.where(dist_mat == np.min(dist_mat))[0])\n",
        "    classes = Y_train[index]\n",
        "    pred_class.append(classes)\n",
        "  return pred_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6MCVcL5-1BD"
      },
      "source": [
        "def use_val(train_X,train_Y,val_X,val_Y,model):\n",
        "  pred = test_predict(train_X,train_Y,val_X,model)\n",
        "  pred = to_categorical(pred)\n",
        "  cm,oa,kappa = get_cm_oa_kappa(val_Y,pred)\n",
        "  return oa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epvExc2jzIbw"
      },
      "source": [
        "\n",
        "# weight_='/content/gdrive/MyDrive/CM/weight_folder/zhangbei_triplet_weight_alpha=0.h5'\n",
        "\n",
        "X_train = np.load('/content/gdrive/MyDrive/CM/roi_patches/zb_train_X_r=5.npy')\n",
        "Y_train = np.load('/content/gdrive/MyDrive/CM/roi_patches/zb_train_Y_r=5.npy')\n",
        "X_val = np.load('/content/gdrive/MyDrive/CM/roi_patches/zb_val_X_r=5.npy')\n",
        "Y_val = np.load('/content/gdrive/MyDrive/CM/roi_patches/zb_val_Y_r=5.npy')\n",
        "X_test = np.load('/content/gdrive/MyDrive/CM/roi_patches/zb_test_X_r=5.npy')\n",
        "Y_test = np.load('/content/gdrive/MyDrive/CM/roi_patches/zb_test_Y_r=5.npy')\n",
        "unlabeled = np.load('/content/gdrive/MyDrive/CM/roi_patches/zhangbei_unlabeled_X.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Emqq8v9z7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4159bd0-ff5f-445e-b7bb-e0346b5f998e"
      },
      "source": [
        "Smodel=feature_extraction_CNN_tri(inputshape)\n",
        "# Smodel.load_weights(weight_,by_name=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "输出向量维度：(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a8e6kDG-QAr",
        "outputId": "24fbf0e5-b24d-4642-a151-e969d23bd69e"
      },
      "source": [
        "num_times = 20 #训练num_times次，每次epoch轮\n",
        "P_weight1=0.0\n",
        "# semimodel = RPmodel(Smodel,alpha=0.2,P_weight = P_weight1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256)\n",
            "(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJMBVq8dzOD8"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "def get_OA(cm):\n",
        "    \n",
        "    total = cm.sum()\n",
        "    \n",
        "    diag_arr = np.diagonal(cm, offset=0)\n",
        "    \n",
        "    correct = diag_arr.sum()\n",
        "    \n",
        "    OA = (correct/total) * 100\n",
        "    \n",
        "    return OA\n",
        "\n",
        "def get_cm_oa_kappa(Y_test, Y_pred,threshold=0.8):\n",
        "    # pred_y=[]\n",
        "    # print(threshold)\n",
        "    # for i in Y_pred:\n",
        "    #   # print(i[1])\n",
        "   \n",
        "    #   if i[1]>=threshold:\n",
        "    #     temp_y=Y_pred[i]\n",
        "    #     pred_y.append(temp_y)\n",
        "    #   else:\n",
        "    #     temp_y=0\n",
        "    #     pred_y.append(temp_y)\n",
        "    # # print(Y_test,Y_pred)\n",
        "    # # print(pred_y)\n",
        "    # cm = confusion_matrix(pred_y, Y_test)\n",
        "    # oa = get_OA(cm)\n",
        "    # kappa = cohen_kappa_score(pred_y, Y_test)\n",
        "\n",
        "    ###############################\n",
        "    Y_pred = np.argmax(Y_pred, axis=1)\n",
        "    \n",
        "    Y_test = Y_test.tolist()\n",
        "    Y_pred = Y_pred.tolist()\n",
        "    # print(Y_pred)\n",
        "    # print('\\n')\n",
        "    # print(Y_test)\n",
        "  \n",
        "    cm = confusion_matrix(Y_pred, Y_test)\n",
        "    \n",
        "    oa = get_OA(cm)\n",
        "    \n",
        "    kappa = cohen_kappa_score(Y_pred, Y_test)\n",
        "    \n",
        "    # print(cm)\n",
        "    \n",
        "    return cm, oa, kappa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnVAV-tuMlI"
      },
      "source": [
        "train_model(Smodel,X_train,Y_train,unlabeled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxhZzlXt_8tO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28cece76-9f69-4cf7-8afe-2093ad596c5b"
      },
      "source": [
        "#############测试模型\n",
        "Tmodel=feature_extraction_CNN_tri(inputshape)\n",
        "weight_ = '/content/gdrive/MyDrive/CM/weight_folder/zhangbei_triplet_weight_plus_pseudo_alpha=0.h5'\n",
        "Tmodel.load_weights(weight_,by_name=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "输出向量维度：(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q32jbH4T4xvd"
      },
      "source": [
        "pred = test_predict(X_train,Y_train,X_test,Tmodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFj7dBGL4zZp"
      },
      "source": [
        "pred1 = to_categorical(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh1DiibM44r2",
        "outputId": "c018d25c-7390-4ee2-d96b-27d4793ef468"
      },
      "source": [
        "cm, oa, kappa = get_cm_oa_kappa(Y_test, pred1)\n",
        "print('acc: {:.2f}%  Kappa: {:.4f}'.format(oa,kappa))\n",
        "# print('acc: {:2f}%'.format(oa))\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 82.92%  Kappa: 0.7961\n",
            "[[73  5  1  0  0  0  0]\n",
            " [12 85  1  2  0  0  0]\n",
            " [ 5  0 88  0  3  7  0]\n",
            " [ 0  0  0 47  0  0  3]\n",
            " [ 0  0  0  0 55 10  1]\n",
            " [ 0  0  0  3 32 73  0]\n",
            " [ 0  0  0  4  0  0 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoVH51y45DWg",
        "outputId": "80678a99-ef1d-4a64-f67e-47957c7a10af"
      },
      "source": [
        "for i in range(50):\n",
        "  Tmodel=feature_extraction_CNN_tri(inputshape)\n",
        "  weight_ = '/content/gdrive/MyDrive/CM/weight_folder/zhangbei_triplet_weight_alpha=0.h5'\n",
        "  Tmodel.load_weights(weight_,by_name=True)\n",
        "  pred = test_predict(X_train,Y_train,X_test,Tmodel)\n",
        "  pred1 = to_categorical(pred)\n",
        "  cm, oa, kappa = get_cm_oa_kappa(Y_test, pred1)\n",
        "  print('acc: {:.2f}%  Kappa: {:.4f}'.format(oa,kappa))\n",
        "  # print('acc: {:2f}%'.format(oa))\n",
        "  print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "输出向量维度：(None, 256)\n",
            "acc: 82.53%  Kappa: 0.7915\n",
            "[[67  9  0  0  0  0  0]\n",
            " [18 81  0  2  0  0  0]\n",
            " [ 5  0 89  0  7 10  0]\n",
            " [ 0  0  1 52  0  0  1]\n",
            " [ 0  0  0  0 59 10  2]\n",
            " [ 0  0  0  1 24 70  0]\n",
            " [ 0  0  0  1  0  0 12]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.77%  Kappa: 0.7822\n",
            "[[71  7  0  0  0  0  0]\n",
            " [17 83  3  2  0  0  0]\n",
            " [ 2  0 87  0  8  7  0]\n",
            " [ 0  0  0 48  0  0  0]\n",
            " [ 0  0  0  0 56 13  4]\n",
            " [ 0  0  0  3 26 70  0]\n",
            " [ 0  0  0  3  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.15%  Kappa: 0.7866\n",
            "[[68  5  0  0  0  0  0]\n",
            " [18 85  2  5  0  0  0]\n",
            " [ 4  0 88  0  7 10  0]\n",
            " [ 0  0  0 49  0  0  4]\n",
            " [ 0  0  0  0 60  9  4]\n",
            " [ 0  0  0  0 23 71  0]\n",
            " [ 0  0  0  2  0  0  7]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.85%  Kappa: 0.7592\n",
            "[[61 10  0  1  0  0  0]\n",
            " [27 80  3  1  0  0  0]\n",
            " [ 2  0 87  0  9  9  0]\n",
            " [ 0  0  0 52  0  0  0]\n",
            " [ 0  0  0  0 49  5  4]\n",
            " [ 0  0  0  2 32 76  0]\n",
            " [ 0  0  0  0  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.73%  Kappa: 0.7936\n",
            "[[75  8  0  0  0  0  0]\n",
            " [12 82  1  2  0  0  0]\n",
            " [ 3  0 89  0  5  8  0]\n",
            " [ 0  0  0 50  0  0  2]\n",
            " [ 0  0  0  0 54 12  2]\n",
            " [ 0  0  0  3 31 70  0]\n",
            " [ 0  0  0  1  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.34%  Kappa: 0.7890\n",
            "[[71  6  0  0  0  0  0]\n",
            " [16 84  1  3  0  0  0]\n",
            " [ 3  0 89  1  2  8  0]\n",
            " [ 0  0  0 48  0  0  3]\n",
            " [ 0  0  0  0 58 12  3]\n",
            " [ 0  0  0  1 30 70  0]\n",
            " [ 0  0  0  3  0  0  9]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.77%  Kappa: 0.7824\n",
            "[[72 10  0  1  0  0  0]\n",
            " [15 80  2  2  0  0  0]\n",
            " [ 3  0 88  0  8  7  0]\n",
            " [ 0  0  0 47  0  0  0]\n",
            " [ 0  0  0  0 54 12  1]\n",
            " [ 0  0  0  2 28 71  0]\n",
            " [ 0  0  0  4  0  0 14]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.08%  Kappa: 0.7499\n",
            "[[67  8  0  0  0  0  0]\n",
            " [20 82  1  2  0  0  0]\n",
            " [ 3  0 88  0 10  8  0]\n",
            " [ 0  0  1 48  0  0  4]\n",
            " [ 0  0  0  0 48 11  3]\n",
            " [ 0  0  0  4 32 71  0]\n",
            " [ 0  0  0  2  0  0  8]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.23%  Kappa: 0.7637\n",
            "[[67 10  0  0  0  0  0]\n",
            " [21 80  1  3  0  0  0]\n",
            " [ 2  0 89  0 10 10  0]\n",
            " [ 0  0  0 50  0  0  4]\n",
            " [ 0  0  0  0 54 10  3]\n",
            " [ 0  0  0  1 26 70  0]\n",
            " [ 0  0  0  2  0  0  8]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.73%  Kappa: 0.7938\n",
            "[[64 10  0  0  0  0  0]\n",
            " [24 80  2  2  0  0  0]\n",
            " [ 2  0 88  0  7  8  0]\n",
            " [ 0  0  0 52  0  0  1]\n",
            " [ 0  0  0  0 58  6  1]\n",
            " [ 0  0  0  1 25 76  0]\n",
            " [ 0  0  0  1  0  0 13]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.81%  Kappa: 0.7710\n",
            "[[67 14  0  1  0  0  0]\n",
            " [21 76  0  1  0  0  0]\n",
            " [ 2  0 89  0 11  7  0]\n",
            " [ 0  0  1 52  0  0  1]\n",
            " [ 0  0  0  0 51 11  0]\n",
            " [ 0  0  0  1 28 72  0]\n",
            " [ 0  0  0  1  0  0 14]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.77%  Kappa: 0.7823\n",
            "[[68  6  0  0  0  0  0]\n",
            " [18 84  2  4  0  0  0]\n",
            " [ 4  0 88  0 10  9  0]\n",
            " [ 0  0  0 47  0  0  0]\n",
            " [ 0  0  0  0 52  6  3]\n",
            " [ 0  0  0  1 28 75  0]\n",
            " [ 0  0  0  4  0  0 12]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 84.45%  Kappa: 0.8144\n",
            "[[74  3  0  1  0  0  0]\n",
            " [14 87  2  2  0  0  0]\n",
            " [ 2  0 88  0  4  9  0]\n",
            " [ 0  0  0 53  0  0  1]\n",
            " [ 0  0  0  0 57 13  1]\n",
            " [ 0  0  0  0 29 68  0]\n",
            " [ 0  0  0  0  0  0 13]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.61%  Kappa: 0.7681\n",
            "[[70 13  1  1  1  0  0]\n",
            " [16 77  2  1  0  1  0]\n",
            " [ 4  0 87  0  9 12  0]\n",
            " [ 0  0  0 51  0  0  4]\n",
            " [ 0  0  0  0 60  8  5]\n",
            " [ 0  0  0  3 20 69  0]\n",
            " [ 0  0  0  0  0  0  6]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.77%  Kappa: 0.7824\n",
            "[[69 11  0  0  0  0  0]\n",
            " [17 79  2  3  0  0  0]\n",
            " [ 4  0 88  0  5 12  0]\n",
            " [ 0  0  0 49  0  0  0]\n",
            " [ 0  0  0  0 57  7  1]\n",
            " [ 0  0  0  2 28 70  0]\n",
            " [ 0  0  0  2  0  1 14]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.23%  Kappa: 0.7639\n",
            "[[74 11  1  2  0  0  0]\n",
            " [13 79  1  1  0  0  0]\n",
            " [ 3  0 88  0 14 13  0]\n",
            " [ 0  0  0 50  0  0  1]\n",
            " [ 0  0  0  0 50 11  3]\n",
            " [ 0  0  0  1 26 66  0]\n",
            " [ 0  0  0  2  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.34%  Kappa: 0.7890\n",
            "[[64  6  0  0  0  0  0]\n",
            " [22 84  2  3  0  0  0]\n",
            " [ 4  0 88  0  7  9  0]\n",
            " [ 0  0  0 53  0  0  3]\n",
            " [ 0  0  0  0 55  5  3]\n",
            " [ 0  0  0  0 28 76  0]\n",
            " [ 0  0  0  0  0  0  9]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 78.50%  Kappa: 0.7431\n",
            "[[69 10  1  1  0  0  0]\n",
            " [19 80  3  1  0  0  0]\n",
            " [ 2  0 86  0 12 11  0]\n",
            " [ 0  0  0 49  0  0  4]\n",
            " [ 0  0  0  0 48 10  3]\n",
            " [ 0  0  0  3 30 69  0]\n",
            " [ 0  0  0  2  0  0  8]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.65%  Kappa: 0.7570\n",
            "[[74 12  0  1  0  0  0]\n",
            " [13 78  3  1  0  0  0]\n",
            " [ 3  0 87  0 16 12  0]\n",
            " [ 0  0  0 50  0  0  1]\n",
            " [ 0  0  0  0 46  9  3]\n",
            " [ 0  0  0  2 28 69  0]\n",
            " [ 0  0  0  2  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 76.39%  Kappa: 0.7179\n",
            "[[64 14  0  0  0  0  0]\n",
            " [22 76  1  2  0  0  0]\n",
            " [ 4  0 89  0 18 15  0]\n",
            " [ 0  0  0 50  0  0  2]\n",
            " [ 0  0  0  0 41  7  3]\n",
            " [ 0  0  0  3 31 68  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.57%  Kappa: 0.7799\n",
            "[[69  8  1  1  0  0  0]\n",
            " [18 82  1  2  0  0  0]\n",
            " [ 3  0 88  0 12 14  0]\n",
            " [ 0  0  0 48  0  0  1]\n",
            " [ 0  0  0  0 59  7  4]\n",
            " [ 0  0  0  2 19 69  0]\n",
            " [ 0  0  0  3  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.85%  Kappa: 0.7592\n",
            "[[69 10  1  1  0  0  0]\n",
            " [17 80  1  3  0  0  0]\n",
            " [ 4  0 88  0 14 11  0]\n",
            " [ 0  0  0 50  0  0  2]\n",
            " [ 0  0  0  0 49  9  3]\n",
            " [ 0  0  0  1 27 70  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.42%  Kappa: 0.7662\n",
            "[[65  4  0  0  0  0  0]\n",
            " [22 86  2  4  0  0  0]\n",
            " [ 3  0 88  0 10  8  0]\n",
            " [ 0  0  0 47  0  0  1]\n",
            " [ 0  0  0  0 48  8  3]\n",
            " [ 0  0  0  1 32 74  0]\n",
            " [ 0  0  0  4  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.38%  Kappa: 0.7776\n",
            "[[71  5  0  1  0  0  0]\n",
            " [16 85  2  1  0  0  0]\n",
            " [ 3  0 88  0 14 13  0]\n",
            " [ 0  0  0 51  0  0  0]\n",
            " [ 0  0  0  0 51 11  3]\n",
            " [ 0  0  0  3 25 66  0]\n",
            " [ 0  0  0  0  0  0 12]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.08%  Kappa: 0.7501\n",
            "[[63 12  1  2  0  0  0]\n",
            " [22 78  0  1  0  0  0]\n",
            " [ 5  0 89  0 13 10  0]\n",
            " [ 0  0  0 50  0  0  2]\n",
            " [ 0  0  0  0 48  7  2]\n",
            " [ 0  0  0  2 29 73  0]\n",
            " [ 0  0  0  1  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.46%  Kappa: 0.7546\n",
            "[[68 12  0  1  0  0  0]\n",
            " [20 78  2  1  0  0  0]\n",
            " [ 2  0 88  0 15  8  0]\n",
            " [ 0  0  0 50  0  0  2]\n",
            " [ 0  0  0  0 47  9  3]\n",
            " [ 0  0  0  3 28 73  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 78.89%  Kappa: 0.7477\n",
            "[[64 10  0  0  0  0  0]\n",
            " [23 80  2  2  0  0  0]\n",
            " [ 3  0 88  0 10  9  0]\n",
            " [ 0  0  0 51  0  0  2]\n",
            " [ 0  0  0  0 49 12  3]\n",
            " [ 0  0  0  2 31 69  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.77%  Kappa: 0.7820\n",
            "[[70  5  1  1  0  0  0]\n",
            " [18 85  1  1  0  0  0]\n",
            " [ 2  0 88  1 14  8  0]\n",
            " [ 0  0  0 48  0  0  4]\n",
            " [ 0  0  0  0 52  6  4]\n",
            " [ 0  0  0  2 24 76  0]\n",
            " [ 0  0  0  3  0  0  7]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 84.45%  Kappa: 0.8142\n",
            "[[78  5  0  0  0  0  0]\n",
            " [ 9 85  1  3  0  0  0]\n",
            " [ 3  0 89  0  5  6  0]\n",
            " [ 0  0  0 50  0  0  1]\n",
            " [ 0  0  0  0 57 14  3]\n",
            " [ 0  0  0  2 28 70  0]\n",
            " [ 0  0  0  1  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.08%  Kappa: 0.7504\n",
            "[[62  9  0  0  0  0  0]\n",
            " [25 81  1  4  0  0  0]\n",
            " [ 3  0 89  0 15  9  0]\n",
            " [ 0  0  0 49  0  0  2]\n",
            " [ 0  0  0  0 49 12  0]\n",
            " [ 0  0  0  0 26 69  0]\n",
            " [ 0  0  0  3  0  0 13]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.42%  Kappa: 0.7661\n",
            "[[68 10  0  2  0  0  0]\n",
            " [19 80  2  0  0  0  0]\n",
            " [ 3  0 87  0 10 10  0]\n",
            " [ 0  0  0 52  0  0  1]\n",
            " [ 0  0  0  0 51 10  3]\n",
            " [ 0  0  1  2 29 70  0]\n",
            " [ 0  0  0  0  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.57%  Kappa: 0.7798\n",
            "[[67  5  1  1  0  0  0]\n",
            " [20 85  1  1  0  0  0]\n",
            " [ 3  0 88  0 10 13  0]\n",
            " [ 0  0  0 49  0  0  2]\n",
            " [ 0  0  0  0 55  6  3]\n",
            " [ 0  0  0  3 25 71  0]\n",
            " [ 0  0  0  2  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.19%  Kappa: 0.7754\n",
            "[[64  7  0  0  0  0  0]\n",
            " [23 83  1  5  0  0  0]\n",
            " [ 3  0 88  0  7  8  0]\n",
            " [ 0  0  1 50  0  0  1]\n",
            " [ 0  0  0  0 54 11  1]\n",
            " [ 0  0  0  0 29 71  0]\n",
            " [ 0  0  0  1  0  0 13]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.00%  Kappa: 0.7730\n",
            "[[65 10  0  2  0  0  0]\n",
            " [19 80  1  0  0  0  0]\n",
            " [ 6  0 89  1 10 13  0]\n",
            " [ 0  0  0 52  0  0  2]\n",
            " [ 0  0  0  0 56  7  3]\n",
            " [ 0  0  0  0 24 70  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.85%  Kappa: 0.7592\n",
            "[[68 11  0  0  0  0  0]\n",
            " [19 79  1  4  0  0  0]\n",
            " [ 3  0 89  0  6 11  0]\n",
            " [ 0  0  0 50  0  0  4]\n",
            " [ 0  0  0  0 56 13  3]\n",
            " [ 0  0  0  0 28 66  0]\n",
            " [ 0  0  0  2  0  0  8]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.15%  Kappa: 0.7869\n",
            "[[71 11  0  2  0  0  0]\n",
            " [15 79  0  1  0  0  0]\n",
            " [ 4  0 89  0  7  7  0]\n",
            " [ 0  0  1 50  0  0  2]\n",
            " [ 0  0  0  0 56 12  1]\n",
            " [ 0  0  0  1 27 71  0]\n",
            " [ 0  0  0  2  0  0 12]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.96%  Kappa: 0.7843\n",
            "[[74 10  0  0  0  0  0]\n",
            " [13 80  3  2  0  0  0]\n",
            " [ 3  0 87  0  3 11  0]\n",
            " [ 0  0  0 52  0  0  5]\n",
            " [ 0  0  0  0 57  8  4]\n",
            " [ 0  0  0  1 30 71  0]\n",
            " [ 0  0  0  1  0  0  6]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.34%  Kappa: 0.7890\n",
            "[[71 11  0  1  0  0  0]\n",
            " [15 79  2  3  0  1  0]\n",
            " [ 4  0 88  0  5  6  0]\n",
            " [ 0  0  0 50  0  0  2]\n",
            " [ 0  0  0  0 55  8  2]\n",
            " [ 0  0  0  1 30 75  0]\n",
            " [ 0  0  0  1  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 76.39%  Kappa: 0.7178\n",
            "[[66 17  2  0  0  0  0]\n",
            " [20 73  1  2  0  0  0]\n",
            " [ 4  0 87  0 11 12  0]\n",
            " [ 0  0  0 50  0  0  3]\n",
            " [ 0  0  0  0 45 10  3]\n",
            " [ 0  0  0  3 34 68  0]\n",
            " [ 0  0  0  1  0  0  9]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 83.69%  Kappa: 0.8051\n",
            "[[69  5  0  2  0  0  0]\n",
            " [18 85  2  1  0  0  0]\n",
            " [ 3  0 88  0  3 10  0]\n",
            " [ 0  0  0 49  0  0  1]\n",
            " [ 0  0  0  0 61  7  3]\n",
            " [ 0  0  0  2 26 73  0]\n",
            " [ 0  0  0  2  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.00%  Kappa: 0.7731\n",
            "[[71 11  0  0  0  0  0]\n",
            " [16 79  1  3  0  0  0]\n",
            " [ 3  0 88  0  6  8  0]\n",
            " [ 0  0  1 50  0  0  3]\n",
            " [ 0  0  0  0 51 10  1]\n",
            " [ 0  0  0  1 33 72  0]\n",
            " [ 0  0  0  2  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 76.97%  Kappa: 0.7249\n",
            "[[65 12  0  1  0  0  1]\n",
            " [24 78  3  3  0  0  0]\n",
            " [ 1  0 87  0 13 12  0]\n",
            " [ 0  0  0 48  0  0  2]\n",
            " [ 0  0  0  0 44 10  1]\n",
            " [ 0  0  0  1 33 68  0]\n",
            " [ 0  0  0  3  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 80.81%  Kappa: 0.7709\n",
            "[[71 10  0  1  0  0  0]\n",
            " [14 80  1  4  0  1  0]\n",
            " [ 5  0 88  0 11  8  0]\n",
            " [ 0  0  1 49  0  0  2]\n",
            " [ 0  0  0  0 51 11  1]\n",
            " [ 0  0  0  0 28 70  0]\n",
            " [ 0  0  0  2  0  0 12]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 82.73%  Kappa: 0.7936\n",
            "[[70 10  0  2  0  0  0]\n",
            " [17 80  2  0  0  0  0]\n",
            " [ 3  0 88  1  4  5  0]\n",
            " [ 0  0  0 51  0  0  1]\n",
            " [ 0  0  0  0 55  9  3]\n",
            " [ 0  0  0  2 31 76  0]\n",
            " [ 0  0  0  0  0  0 11]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.85%  Kappa: 0.7591\n",
            "[[66  6  0  1  0  0  0]\n",
            " [21 84  0  1  0  0  0]\n",
            " [ 3  0 89  0  9  8  0]\n",
            " [ 0  0  1 50  0  0  3]\n",
            " [ 0  0  0  0 50 13  4]\n",
            " [ 0  0  0  3 31 69  0]\n",
            " [ 0  0  0  1  0  0  8]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 75.62%  Kappa: 0.7084\n",
            "[[60 12  0  0  0  0  0]\n",
            " [25 78  1  3  0  0  0]\n",
            " [ 5  0 89  1 18  9  0]\n",
            " [ 0  0  0 49  0  0  4]\n",
            " [ 0  0  0  0 39  9  4]\n",
            " [ 0  0  0  3 33 72  0]\n",
            " [ 0  0  0  0  0  0  7]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 79.46%  Kappa: 0.7546\n",
            "[[65  8  1  0  0  0  0]\n",
            " [22 82  2  2  0  0  0]\n",
            " [ 3  0 87  0 15 11  0]\n",
            " [ 0  0  0 51  1  0  1]\n",
            " [ 0  0  0  0 46  6  4]\n",
            " [ 0  0  0  2 28 73  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 83.69%  Kappa: 0.8051\n",
            "[[75  5  1  2  0  0  0]\n",
            " [13 85  0  0  0  0  0]\n",
            " [ 2  0 88  0 11  8  0]\n",
            " [ 0  0  1 50  0  0  4]\n",
            " [ 0  0  0  0 57 11  1]\n",
            " [ 0  0  0  3 22 71  0]\n",
            " [ 0  0  0  1  0  0 10]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 81.57%  Kappa: 0.7801\n",
            "[[69  8  1  0  0  0  0]\n",
            " [18 82  1  2  0  0  0]\n",
            " [ 3  0 88  1 14 10  0]\n",
            " [ 0  0  0 50  0  0  0]\n",
            " [ 0  0  0  0 51  9  1]\n",
            " [ 0  0  0  1 25 71  0]\n",
            " [ 0  0  0  2  0  0 14]]\n",
            "输出向量维度：(None, 256)\n",
            "acc: 77.74%  Kappa: 0.7340\n",
            "[[64 11  1  0  0  0  0]\n",
            " [23 79  2  2  0  0  0]\n",
            " [ 3  0 86  0 14  9  0]\n",
            " [ 0  0  1 46  0  0  2]\n",
            " [ 0  0  0  0 50  9  5]\n",
            " [ 0  0  0  3 26 72  0]\n",
            " [ 0  0  0  5  0  0  8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVG18AsIDUoE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}